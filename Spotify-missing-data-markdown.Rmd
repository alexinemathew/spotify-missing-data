---
title: 'Missing Data: Final Project'
author: Alexine Mathew
date: 3/12/2022
output:
  pdf_document
geometry: margin=0.25in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
library(mice)
library(mi)
library(VIM)
library(dplyr)
library(naniar)
library(tidyverse)
library(ggcorrplot)
library(formatR)
library(repr)
library(kableExtra)
```

### 1. Intro 
#### Dataset: 
The original dataset is called "Spotify Top 200 Charts (2020-2021)" and can be found [$\text{\underline{here}}$](https://www.kaggle.com/sashankpillai/spotify-top-200-charts-20202021). 

The dataset include all the songs that have been on the Top 200 Weekly (Global) charts of Spotify in 2020 & 2021.
I have subset the original dataset to only include the following variables: 

* **Number of Times Charted:** The number of times that the song has been on in the Spotify Top 200 Weekly Global Charts in 2020 & 2021.
* **Artist Followers:** The number of followers the main artist has on Spotify.
* **Popularity:** The popularity of the track. The value will be between 0 and 100, with 100 being the most popular.
* **Tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.
* **Duration:** The duration of the song in milliseconds.
* **Feature:** This is a column that I created based on whether on not there are multiple artists (aka guest features) on the track. This is a binary value (coding is 1 if there are guest features, 0 if none).

This dataset is completely observed with 1,545 observations, so I will have to manually create missingness. 

#### Model  
* **Model:** Linear regression 
* **IV:** Popularity, Tempo, Artist.Followers, Feature, Duration.ms
* **DV:** Number.of.Times.Charted

**Research question:** Do song popularity, tempo, number of artist followers, guest features, and song duration have an influence on the number of times that a song is charted? In other words, we are looking to estimate how many times a song will be charted (Number.of.Times.Charted) based on the following five factors: popularity, tempo, number of artists followers, guest features, and duration. 
```{r}
# original dataset - no missingness
spotify <- read.csv("spotify.csv")
set.seed(123)
nrow(spotify) # 1545 observations 

# x1: Popularity - numeric
# x2: Tempo - numeric
# x3: feature - dichotomous
# x4: Artist.Followers - numeric, scaled  
# x5: Duration..ms - numeric, scaled 
# y: Number.of.Times.Charted - numeric

# regression summary - this is our benchmark against other models 
reg <- lm(Number.of.Times.Charted ~ Popularity + Tempo + scale(Artist.Followers) + as.factor(Feature) + scale(Duration.ms), data=spotify)
summary(reg)
# SEs
reg.sd <- coef(summary(reg))[, "Std. Error"]
reg.sd
```
I scaled the variables Artist.Followers and Duration.ms to have them on comparable magnitudes to the rest of the variables. 
```
Estimate equation: Number.of.Times.Charted = -3.14907 + 0.24029*Popularity 
- 0.02295*Tempo - 0.06600*Artist.Followers -0.81121*Factor + 0.27799*Duration.ms
```
If we use an alpha level of 0.05 to determine which predictors are significant in this model, we would say that only Popularity is statistically significant. 

```{r}
# generate MAR missingness 
set.seed(123)
spotify.miss = ampute(spotify, prop = 0.6, mech = "MAR")$amp
# leave one numerical variable as completely observed (Duration.ms)
spotify.miss = cbind(spotify.miss[,1:5],spotify[,6])
colnames(spotify.miss)[6] = "Duration.ms"
# refactor Feature as factor for new df with missingness
spotify.miss$Feature <- as.factor(spotify.miss$Feature)
```
### 2. Summary Statistics 
```{r}
# summary statistics  
summary(spotify.miss)
# histograms for numerical variables 
hist <- data.frame(spotify.miss$Number.of.Times.Charted, spotify.miss$Popularity, spotify.miss$Tempo, spotify.miss$Artist.Followers, spotify.miss$Duration.ms)
Hmisc::hist.data.frame(hist) # obvious skews for Number.of.Times.Charted and Artist.Followers
# boxplot for categorical variable
# distribution of 0 and 1 seems similar, outliers in 0 (no feature)
boxplot(Number.of.Times.Charted~Feature,data=spotify.miss,
   xlab="Feature", ylab="Number.of.Times.Charted",col=c("orange" , "seagreen"))  
# % missing per variable
miss_var_summary(spotify.miss) 
# percent complete cases
colMeans(!is.na(spotify.miss))
```
### 3. Mean/Mode Imputation
```{r}
# numerical variables 
str(spotify)
# copy dataset with missingness 
spotify.imputed <- spotify.miss
# mean imputation function
# The input is a data vector to be imputed 
mean.imp <- function (a)
  {
  missing <- is.na(a)
  a.obs <- a[!missing]
  imputed <- a
  imputed[missing] <- mean(a.obs)
  # Output the imputed vector
  return (imputed)
}
# mean imputation for numerical variables 
spotify.imputed$Popularity <- mean.imp(spotify.imputed$Popularity)
spotify.imputed$Tempo <- mean.imp(spotify.imputed$Tempo)
spotify.imputed$Number.of.Times.Charted <- mean.imp(spotify.imputed$Number.of.Times.Charted)
spotify.imputed$Artist.Followers <- mean.imp(spotify.imputed$Artist.Followers)
```

```{r}
# mode function
mode = function(x)
{
 ta = table(x)
 tam = max(ta)
 if (all(ta == tam))
 mod = NA
 else
 mod = names(ta)[ta == tam]
 return(mod)
}
# mode imputation
mode.imp <- function (a)
{
 missing <- is.na(a)
 a.obs <- a[!missing]
 imputed <- a
 imputed[missing] <- mode(a.obs)
 # Output the imputed vector
 return (imputed)
}
# mode imputation for categorical variable
# Feature 
spotify.imputed$Feature <- mode.imp(spotify.imputed$Feature)
```

```{r}
##  comparing results 
# mean/mode imputed data
set.seed(123)
reg.mean <- lm(Number.of.Times.Charted ~ Popularity + Tempo + scale(Artist.Followers) + Feature + scale(Duration.ms), data=spotify.imputed)
summary(reg.mean)
# total change 
sum(abs(reg.mean$coef - reg$coef)) 
# average percent change in coefficients - absolute terms 
mean(abs(reg.mean$coef - reg$coef)/abs(reg$coef)) 
# SE
reg.mean.sd <- coef(summary(reg.mean))[, "Std. Error"]
# average percent change in SE
mean((reg.mean.sd - reg.sd)/(reg.sd))
# average percent change in SE - absolute terms 
mean(abs(reg.mean.sd - reg.sd)/abs(reg.sd))

```

```{r}
## Density of Tempo,Duration.ms,Number.of.Times.Charted pre and post imputation ##
par(mfrow = c(1, 3))
# observed Tempo
plot(density(spotify$Tempo),
     lwd = 2, 
     ylim = c(0, 0.020),
     main="",
     xlab = "Tempo")
# imputed Tempo
points(density(spotify.imputed$Tempo), 
       lwd = 2, 
       ylim = c(0, 0.020),
       type = "l", 
       col = "red")
# observed Popularity
plot(density(spotify$Popularity),
     lwd = 2, 
     ylim = c(0, 0.05),
     main="",
     xlab = "Popularity")
# imputed Popularity
points(density(spotify.imputed$Popularity), 
       lwd = 2, 
       ylim = c(0, 0.05),
       type = "l", 
       col = "red")
# observed Number.of.Times.Charted
plot(density(spotify$Number.of.Times.Charted),
     lwd = 2, 
     main="",
     xlab = "Number.of.Times.Charted")
# imputed Number.of.Times.Charted
points(density(spotify.imputed$Number.of.Times.Charted), 
       lwd = 2, 
       ylim = c(0, 0.05),
       type = "l", 
       col = "red")
```

```{r}
## Correlations ##
# only numerical variables 
cor.spotify <- data.frame(spotify$Number.of.Times.Charted, spotify$Popularity, spotify$Tempo, spotify$Artist.Followers, spotify$Duration.ms)
cor.spotify.miss <- data.frame(spotify.miss$Number.of.Times.Charted, spotify.miss$Popularity, spotify.miss$Tempo, spotify.miss$Artist.Followers, spotify.miss$Duration.ms)
# correlations between variables in observed and imputed data-sets
r <- data.frame(cor(cor.spotify))
r.miss <- data.frame(cor(cor.spotify.miss, use ="na.or.complete"))
names(r) <- c("Number.of.Times.Charted", "Popularity", "Tempo", "Artist.Followers","Duration.ms")
names(r.miss) <- c("Number.of.Times.Charted", "Popularity", "Tempo", "Artist.Followers","Duration.ms")
r %>%
  kbl(caption = "Observed Dataset") %>%
  kable_classic(full_width = F, html_font = "Cambria")
  
r.miss %>%
  kbl(caption = "Imputed Dataset") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

  
The first imputation method that was used was mean/mode imputation. Mean imputation was used for the numerical variables' missing values and mode was used for the categorical variable, Feature. 
The average percent change in coefficients, compared to complete data, was 46.37%.
The average percent change in SEs, compared to complete data, decreased by 2.59%.

```
Estimate equation: Number.of.Times.Charted = -1.5293 + 0.2118*Popularity 
- 0.0216*Tempo - 0.1397*Artist.Followers - 0.1762*Factor + 0.2249*Duration.ms
```
The plots above display the density of Tempo, Popularity, and Number.of.Times.Charted, before (in black) and after (in red) the mean imputation.
The imputation seemed to create higher or additional peaks in all of these variables.\ 
Against my expectation, while the average % chg in SE decreased, the SE for Tempo actually increased by 0.0001.
As seen on the correlation tables, correlation decreased for all pairs after imputation, except Tempo & Number.of.Times.Charted, Duration.ms & Popularity and Artist.Followers & Popularity. 
A general issue of mean imputation is that it does not preserve the relationships between coefficients, and we can see that illustrated here. 
  
### 4. Random Imputation
```{r}
# copy dataset with missingness
spotify.random <- spotify.miss
# random imputation function 
random.imp <- function (a)
{
  missing <- is.na(a)
  n.missing <- sum(missing)
  a.obs <- a[!missing]
  imputed <- a
  imputed[missing] <- sample(a.obs, n.missing, replace=TRUE)
  return (imputed)
}
# random imputation for all variables 
spotify.random$Popularity <- random.imp(spotify.random$Popularity)
spotify.random$Tempo <- random.imp(spotify.random$Tempo)
spotify.random$Number.of.Times.Charted <- random.imp(spotify.random$Number.of.Times.Charted)
spotify.random$Artist.Followers <- random.imp(spotify.random$Artist.Followers)
spotify.random$Feature <- random.imp(spotify.random$Feature)
```

```{r}
## comparing results 
# imputed data
set.seed(123)
reg.random <- lm(Number.of.Times.Charted ~ Popularity + Tempo + scale(Artist.Followers) + Feature + scale(Duration.ms), data=spotify.random)
summary(reg.random)
# total change 
sum(abs(reg$coef - reg.random$coef)) 
# average percent change in coefficients
mean(abs(reg$coef - reg.random$coef)/abs(reg$coef)) 
# SE
reg.random.sd <- coef(summary(reg.random))[, "Std. Error"]
# average percent change in SE
mean((reg.random.sd - reg.sd)/(reg.sd)) 
# average percent change in SE - absolute terms 
mean(abs(reg.random.sd - reg.sd)/abs(reg.sd)) 
```

```{r}
## Density of Tempo,Duration.ms,Number.of.Times.Charted pre and post random imputation ##
par(mfrow = c(1, 3))
# observed Tempo
plot(density(spotify$Tempo),
     lwd = 2, 
     ylim = c(0, 0.020),
     main="",
     xlab = "Tempo")
# imputed Tempo
points(density(spotify.random$Tempo), 
       lwd = 2, 
       ylim = c(0, 0.020),
       type = "l", 
       col = "red")
# observed Popularity
plot(density(spotify$Popularity),
     lwd = 2, 
     ylim = c(0, 0.05),
     main="",
     xlab = "Popularity")
# imputed Popularity
points(density(spotify.random$Popularity), 
       lwd = 2, 
       ylim = c(0, 0.05),
       type = "l", 
       col = "red")
# observed Number.of.Times.Charted
plot(density(spotify$Number.of.Times.Charted),
     lwd = 2, 
     main="",
     xlab = "Number.of.Times.Charted")
# imputed Number.of.Times.Charted
points(density(spotify.random$Number.of.Times.Charted), 
       lwd = 2, 
       ylim = c(0, 0.05),
       type = "l", 
       col = "red")
# only numerical variables 
cor.random <- data.frame(spotify.random$Number.of.Times.Charted, spotify.random$Popularity, spotify.random$Tempo, spotify.random$Artist.Followers, spotify.random$Duration.ms)
# correlations between variables in observed and imputed data-sets
r.random <- data.frame(cor(cor.random, use ="na.or.complete"))
names(r.random) <- c("Number.of.Times.Charted", "Popularity", "Tempo", "Artist.Followers","Duration.ms")
r %>%
  kbl(caption = "Observed Dataset Correlations") %>%
  kable_classic(full_width = F, html_font = "Cambria")
  
r.random %>%
  kbl(caption = "Imputed Dataset Correlations") %>%
  kable_classic(full_width = F, html_font = "Cambria") 
```

\
The next imputation method that was used was random imputation, which randomly samples from observed data.
The average percent change in coefficients was 75.4%, which was higher than mean/mode imputation. 
SEs decreased for all variables, with an overall average decrease of 2.48%. 
```
Estimate equation: Number.of.Times.Charted = -1.8862 + 0.2007*Popularity 
-0.0131*Tempo -0.2567*Artist.Followers -0.4603*Factor + 0.2207*Duration.ms
```
As expected, the density plots of random imputation almost perfectly match that of the observed data. 
The correlations changed slightly for most of the pairs, which implies that random imputation (like mean imputation) can also ignore relationships between variables.\


### 5. Hotdecking
```{r}
# copy dataset with missingness
spotify.hotdeck <- spotify.miss
# hotdecking 
spotify.hotdeck <- hotdeck(spotify.hotdeck, imp_var = FALSE)
```

```{r}
##  comparing results 
# imputed data
set.seed(123)
reg.hotdeck <- lm(Number.of.Times.Charted ~ Popularity + Tempo + scale(Artist.Followers) + Feature + scale(Duration.ms), data=spotify.hotdeck)
summary(reg.hotdeck)
# total change 
sum(abs(reg$coef - reg.hotdeck$coef)) 
# average percent change in coefficients
mean(abs(reg$coef - reg.hotdeck$coef)/abs(reg$coef)) 
# SE
reg.hotdeck.sd <- coef(summary(reg.hotdeck))[, "Std. Error"] 
# average percent change in SE
mean((reg.hotdeck.sd - reg.sd)/(reg.sd)) 
# average percent change in SE - absolute terms
mean(abs(reg.hotdeck.sd - reg.sd)/abs(reg.sd))
```

```{r}
## Density of Tempo,Duration.ms,Number.of.Times.Charted pre and post imputation ##
par(mfrow = c(1, 3))
# observed Tempo
plot(density(spotify$Tempo),
     lwd = 2, 
     ylim = c(0, 0.020),
     main="",
     xlab = "Tempo")
# imputed Tempo
points(density(spotify.hotdeck$Tempo), 
       lwd = 2, 
       ylim = c(0, 0.020),
       type = "l", 
       col = "red")
# observed Popularity
plot(density(spotify$Popularity),
     lwd = 2, 
     ylim = c(0, 0.05),
     main="",
     xlab = "Popularity")
# imputed Popularity
points(density(spotify.hotdeck$Popularity), 
       lwd = 2, 
       ylim = c(0, 0.05),
       type = "l", 
       col = "red")
# observed Artist.Followers
plot(density(spotify$Artist.Followers),
     lwd = 2, 
     main="",
     xlab = "Artist.Followers")
# imputed Artist.Followers
points(density(spotify.hotdeck$Artist.Followers), 
       lwd = 2, 
       #ylim = c(0, 0.05),
       type = "l", 
       col = "red")
```

The next imputation method that was used was hotdecking, which uses information that is available in the data (within other variables) to “fill in”
information that is missing for a given predictor. Using hotdecking, the average percent change in coefficients was 149.46%, which was significantly higher than prior methods. It seem that the main issue is the Artist.Followers coefficient, which increased by > 500%. SEs decreased for all variables, with an overall average decrease of 3.35%.\
As expected, the density plots of the hotdecking imputations almost perfectly match that of the observed data. 
Interestingly, Tempo now became statistically significant at the 0.05 alpha level. 

### 6. Regression Imputation
```{r}
# copy dataset with missingness
spotify.reg <- spotify.miss
##  linear regression
  # Popularity 
  Popularity.imp = mice.impute.norm.predict(spotify.reg$Popularity, !is.na(spotify.reg$Popularity), spotify.reg$Duration.ms)
  spotify.reg$Popularity[is.na(spotify.reg$Popularity)] = Popularity.imp
  # Tempo
  Tempo.imp = mice.impute.norm.predict(spotify.reg$Tempo, !is.na(spotify.reg$Tempo), spotify.reg$Duration.ms)
  spotify.reg$Tempo[is.na(spotify.reg$Tempo)] = Tempo.imp
  # Artist.Followers
  Artist.Followers.imp = mice.impute.norm.predict(spotify.reg$Artist.Followers, !is.na(spotify.reg$Artist.Followers), spotify.reg$Duration.ms)
  spotify.reg$Artist.Followers[is.na(spotify.reg$Artist.Followers)] = Artist.Followers.imp
  # Number.of.Times.Charted
  Number.of.Times.Charted.imp = mice.impute.norm.predict(spotify.reg$Number.of.Times.Charted, !is.na(spotify.reg$Number.of.Times.Charted), spotify.reg$Duration.ms)
  spotify.reg$Number.of.Times.Charted[is.na(spotify.reg$Number.of.Times.Charted)] = Number.of.Times.Charted.imp
## logistic for categorical variable Feature
    # logreg - manual
    Ry = as.numeric(!is.na(spotify.reg$Feature))
    spotify.reg.cc = spotify.reg[Ry == 1, ]
    spotify.reg.dropped = spotify.reg[Ry == 0, ]
    # Now build the logistic model:
    mylogit <- glm(Feature ~ Popularity + Tempo + scale(Artist.Followers) + scale(Duration.ms) + Number.of.Times.Charted, data = spotify.reg.cc, family = "binomial")
    # We now use the model to predict the missing
    y.imp <- predict(mylogit, newdata = spotify.reg.dropped, type = "response")
    # Note this returns the probability so we need to convert it to binary response before imputing it:
    # without noise
    spotify.reg$Feature[Ry == 0] = round(y.imp,0)

```

```{r}
# regression analysis 
set.seed(123)
reg.reg <- lm(Number.of.Times.Charted ~ Popularity + Tempo + scale(Artist.Followers) + Feature + scale(Duration.ms), data=spotify.reg)
summary(reg.reg)
# total change 
sum(abs(reg$coef - reg.reg$coef)) 
# average percent change in coefficients
mean((abs(reg$coef - reg.reg$coef))/abs(reg$coef)) 
# SE
reg.reg.sd <- coef(summary(reg.reg))[, "Std. Error"]
reg.sd
reg.reg.sd
# average percent change in SE - absolute terms
mean(abs(reg.reg.sd - reg.sd)/abs(reg.sd)) 
```
Using regression imputation (without noise), the average percent change in coefficients was 37.04%. 
SEs decreased for all variables except Tempo; SEs had an overall average percent change of 2.68%. 
Popularity was the only statistically significant variable, at the 0.05 alpha level. 

### 7. Regression imputation with noise on all variables (numerical, dichotomous and multinomial).
```{r}
# copy dataset with missingness
spotify.noise <- spotify.miss
## linear regression for numerical variables 
# norm.nob for noise 
  # Popularity 
  Popularity.imp = mice.impute.norm.nob(spotify.noise$Popularity, !is.na(spotify.noise$Popularity), spotify$Duration.ms)
  spotify.noise$Popularity[is.na(spotify.noise$Popularity)] = Popularity.imp
  # Tempo
  Tempo.imp = mice.impute.norm.nob(spotify.noise$Tempo, !is.na(spotify.noise$Tempo), spotify$Duration.ms)
  spotify.noise$Tempo[is.na(spotify.noise$Tempo)] = Tempo.imp
  # Artist.Followers
  Artist.Followers.imp = mice.impute.norm.nob(spotify.noise$Artist.Followers, !is.na(spotify.noise$Artist.Followers), spotify$Duration.ms)
  spotify.noise$Artist.Followers[is.na(spotify.noise$Artist.Followers)] = Artist.Followers.imp
  # Number.of.Times.Charted
  Number.of.Times.Charted.imp = mice.impute.norm.nob(spotify.noise$Number.of.Times.Charted, !is.na(spotify.noise$Number.of.Times.Charted), spotify$Duration.ms)
  spotify.noise$Number.of.Times.Charted[is.na(spotify.noise$Number.of.Times.Charted)] = Number.of.Times.Charted.imp
## logistic regression for categorical variable Feature
  # logreg - manual
    Ry2 = as.numeric(!is.na(spotify.noise$Feature))
    spotify.noise.cc = spotify.noise[Ry == 1, ]
    spotify.noise.dropped = spotify.noise[Ry == 0, ]
    # Now build the logistic model:
    mylogit2 <- glm(Feature ~ Popularity + Tempo + scale(Artist.Followers) + Duration.ms + Number.of.Times.Charted, data = spotify.noise.cc, family = "binomial")
    # We now use the model to predict the missing
    y.imp2 <- predict(mylogit2, newdata = spotify.noise.dropped, type = "response")
    # Note this returns the probability so we need to convert it to binary response before imputing it:
    # with noise
    spotify.noise$Feature[Ry2 == 0] = rbinom(sum(Ry2==0), 1, y.imp2)
```

```{r}
# regression analysis 
set.seed(123)
reg.noise <- lm(Number.of.Times.Charted ~ Popularity + Tempo + scale(Artist.Followers) + Feature + scale(Duration.ms), data=spotify.noise)
summary(reg.noise)
# total change 
sum(abs(reg$coef - reg.noise$coef)) 
# average percent change in coefficients
mean(abs(reg$coef - reg.noise$coef)/abs(reg$coef)) 
# SE
reg.noise.sd <- coef(summary(reg.noise))[, "Std. Error"]
# average percent change in SE
mean((reg.noise.sd - reg.sd)/(reg.sd)) 
# average percent change in SE - absolute terms 
mean(abs(reg.noise.sd - reg.sd)/abs(reg.sd)) 
```

```{r}
# only numerical variables 
gg.cor.spotify.noise<- data.frame(spotify.noise$Number.of.Times.Charted, spotify.noise$Popularity, spotify.noise$Tempo, spotify.noise$Artist.Followers, spotify.noise$Duration.ms)
# correlations between variables in observed and imputed data-sets
r.noise <- cor(gg.cor.spotify.noise, use ="na.or.complete")
# plots 
```

Regression imputation with noise adds back in most of the variability that standard regression imputation removes. 
The average percent change in coefficients was 35.02%. 
The average percent change in SEs decreased by 0.91%, which is in line with the expectation of adding more variability bad to the model. 
Average % change in abolute terms, was 1.15% for SE. 


### 8. Multiple imputation
```{r}
# Create a missing_data object, then look at the data and the missing data patterns
mdf = missing_data.frame(spotify.miss)
summary(mdf)
image(mdf)
hist(mdf) # Number.of.Times.Charted and Artist.Followers skewed, Popularity has outliers that are negative 
# missing data patterns
tabulate(mdf@patterns)
levels(mdf@patterns)
```

```{r}
# check  data types and methods
show(mdf) 
# changing Number.of.Times.Charted, Artist.Followers  to type positive-contiguous
# does not make sense for observations these variables to be <= 0  
 mdf <- change(mdf, y = c("Number.of.Times.Charted", "Artist.Followers"), what = "type", to = "positive-continuous")
 show(mdf)
```

```{r, include=FALSE}
# mi with 5 chains and 60 iterations
set.seed(123)
imputations <- mi(mdf, n.iter=60, n.chain=5, parallel = F)
```

```{r, include=FALSE}
# convergence
(converged = mi2BUGS(imputations))
```

```{r}
par(mfrow = c(2, 3))
# traceplots 
# traceplot - number of times charted 
mean_Number.of.Times.Charted = converged[, , 1]
ts.plot(mean_Number.of.Times.Charted[,1], col=1)
lines(mean_Number.of.Times.Charted[,2], col= 2)
lines(mean_Number.of.Times.Charted[,3], col= 3)
lines(mean_Number.of.Times.Charted [,4], col= 4)
# traceplot - Popularity
mean_Popularity = converged[, , 2]
ts.plot(mean_Popularity[,1], col=1)
lines(mean_Popularity[,2], col= 2)
lines(mean_Popularity[,3], col= 3)
lines(mean_Popularity[,4], col= 4)
# traceplot - Tempo
mean_Tempo = converged[, , 3]
ts.plot(mean_Tempo[,1], col=1)
lines(mean_Tempo[,2], col= 2)
lines(mean_Tempo[,3], col= 3)
lines(mean_Tempo [,4], col= 4)
# traceplot - Feature
mean_Feature = converged[, , 4]
ts.plot(mean_Feature[,1], col=1)
lines(mean_Feature[,2], col= 2)
lines(mean_Feature[,3], col= 3)
lines(mean_Feature [,4], col= 4)
# traceplot - Artist.Followers
mean_Artist.Followers = converged[, , 5]
ts.plot(mean_Artist.Followers[,1], col=1)
lines(mean_Artist.Followers[,2], col= 2)
lines(mean_Artist.Followers[,3], col= 3)
lines(mean_Artist.Followers [,4], col= 4)
# traceplot - Duration.ms
mean_Duration.ms= converged[, , 5]
ts.plot(mean_Duration.ms[,1], col=1)
lines(mean_Duration.ms[,2], col= 2)
lines(mean_Duration.ms[,3], col= 3)
lines(mean_Duration.ms [,4], col= 4)
```

Convergence looks good for all variables so I will not continue with more imputations. 
```{r}
# check r-hats
Rhats(imputations) 
```

R-hats are ~1 for most variables so I will not make further changes to the imputations. 
```{r}
# plot diagnostics 
plot(imputations)
```

```{r}
# histograms of imputations 
hist(imputations)
```

```{r}
## run pool analysis 
set.seed(123, sample.kind = "Rounding")
mipoolresult <- pool(Number.of.Times.Charted ~ Popularity + Tempo + scale(Artist.Followers) + as.factor(Feature) + scale(Duration.ms), imputations, m = 5)
display(mipoolresult)
# average percent change in coefficients
mean(abs(reg$coef - coef(mipoolresult))/abs(reg$coef)) 
# SEs
pool.sd <- coef(summary(mipoolresult))[, "Std. Error"]
# average percent change in SE
mean(abs(reg.sd - pool.sd)/abs(reg.sd)) 
```

When checking data types, I decided to change the variables Number.of.Times.Charted and Artist.Followers to type positive-contiguous, as it would not not make sense for these observations to be <= 0. The original model families seemed appropriate. I then ran the multiple imputation with 5 chains and 60 iterations. The convergence traceplots looked reasonable for all variables so I decided to not continue with more imputations. R-hats were approximately 1 for all variables so I did not make further changes to the imputations. The diagnostic plots confirmed this decision.\
Multiple imputation with the MI library yielded a 41.12% average percent change in coefficients and a 35.19% average percent change in SE.
SEs appears to increase for most variables (Tempo, Artist.Followers, Feature, Duration.ms).\


### 9. kNN Imputation (VIM)
```{r}
# copy dataset with missingness
spotify.kNN <- spotify.miss

spotify.kNN <- kNN(spotify.kNN, k = 5, imp_var = FALSE)
```

```{r}
# comparing results 
# imputed data
set.seed(123)
reg.kNN <- lm(Number.of.Times.Charted ~ Popularity + Tempo + scale(Artist.Followers) + Feature + scale(Duration.ms), data=spotify.kNN)
summary(reg.kNN)
# total change 
sum(abs(reg$coef - reg.kNN$coef)) 
# average percent change in coefficients
mean(abs(reg$coef - reg.kNN$coef)/abs(reg$coef)) 
# SE
kNN.sd <- coef(summary(reg.kNN))[, "Std. Error"] 
# average percent change in SE
mean(abs(kNN.sd - reg.sd)/abs(reg.sd)) 
```

I was curious to see if the kNN method (which we didn't thoroughly discuss in class) would yield better results. The kNN imputation method uses the kNN algorithm to search the entire dataset for the k number of the most similar cases, or neighbors, that show the same patterns as the row with the missing data.
The average percent change in coefficients was 96.72% and the average percent change in SEs was 3.25%. 




















